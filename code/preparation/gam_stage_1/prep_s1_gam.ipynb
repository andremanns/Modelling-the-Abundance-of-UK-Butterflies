{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5434f1b8-2f04-42c9-8663-fd4b7734bf58",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "Using the optimised model parameters from the GAM stage 1 validation section, to \n",
    "predict missing baseline indices. \n",
    "\n",
    "#### Aim:\n",
    "To compute baseline predictions using the optimised GAM stage 1 models. \n",
    "\n",
    "#### Workflow: \n",
    "1) The dataset is filtered by sample size (A seperate model is optimised for sample\n",
    "sizes 6-8, 9-11, 12-14, 15-17, 18-20).\n",
    "2) The model input parameters are defined (Lambda and spline count)\n",
    "3) The model is created and predictions generated. \n",
    "4) The predictions are stored and exported in csv format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8fa292d-81f1-42a5-b55f-77503c42d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pygam import LinearGAM, s\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Importing localised file directory\n",
    "project_root = Path(os.environ['butterfly_project'])\n",
    "\n",
    "# Importing data\n",
    "gam_1_accept = pd.read_csv(project_root/'Data'/'UKBMS'/'gam_1'/'gam_1_accept.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bcba79-08ea-42f5-ad4e-7256200faea8",
   "metadata": {},
   "source": [
    "#### Log Transforming Site Index Scores using log1p().\n",
    "log1p() is the same as log(x+1). It enables the inclusion of zero values. \n",
    "Indices are transformed due to a large right-skew in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8522c81-1d9f-4815-aea2-bf7959f477e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gam_1_accept['log_site_index'] = np.log1p(gam_1_accept['site_index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8265ea49-bfc5-4196-b3b8-ac564dc5d98f",
   "metadata": {},
   "source": [
    "#### Filtering the Dataset by Sample Size\n",
    "Survey groups are partitioned by the number of consecutive surveys. This is because \n",
    "different sample sizes are likely to require different GAM parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7b7ae2-28dd-44af-a731-95086a20c5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gam_1_accept = (\n",
    "    gam_1_accept[\n",
    "    (gam_1_accept['consecutive_surveys']>=12) # enter min 'consecutive_surveys' here\n",
    "    & (gam_1_accept['consecutive_surveys']<=14) # enter max 'consecutive_surveys' here\n",
    "    ].reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fb9747-c769-4ceb-9f64-d556e1dc23a6",
   "metadata": {},
   "source": [
    "#### Optimal Parameters from Model Optimisation are Defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64d380b-794d-4c73-8c52-fc20b1f6110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lam=10**0.6 # enter optimal lambda value here\n",
    "splines=4 # enter optimal spline count here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deac4a0-a604-4628-b84c-0567d62c6fcf",
   "metadata": {},
   "source": [
    "#### Creating a Baseline Model for All Survey Groups with the Optimal Hyperparameters\n",
    "A single loop is used to run through all the 'consecutive survey groups'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33273b97-3605-4e6a-b67b-ac66a12948bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a unique list of all consecutive survey groups.\n",
    "all_groups = list(gam_1_accept['consecutive_survey_group'].drop_duplicates())\n",
    "# Empty lists are created to store the model output. \n",
    "prediction_data = []\n",
    "edof = []\n",
    "\n",
    "# A consecutive survey subset is acquired by filtering 'cs_dataset' with iterator value\n",
    "for cs_group in all_groups: \n",
    "    cs_dataset = (\n",
    "        gam_1_accept[\n",
    "        gam_1_accept['consecutive_survey_group']==cs_group\n",
    "        ]\n",
    "    )\n",
    "    model_all = (\n",
    "        LinearGAM(s(0, n_splines=splines), lam=lam)\n",
    "        .fit(cs_dataset[['year']], # x-axis years specific to 'cs_dataset'\n",
    "             cs_dataset['log_site_index']) # y-axis indices specific to cs_dataset.\n",
    "    )\n",
    "    # Defining the baseline year in a 1x1 df. This is needed because .predict()\n",
    "    # only accepts 2D input. \n",
    "    explanatory_data = pd.DataFrame([1993], columns=['explanatory_data'])\n",
    "    # using the model to generate a single baseline prediction for each cs_group\n",
    "    prediction_data.append(\n",
    "        model_all.predict(explanatory_data)[0] # [0] extracts value from 1D array\n",
    "    ) \n",
    "    edof.append(model_all.statistics_['edof']) # effective degrees of freedom.\n",
    "\n",
    "# Populated lists are aggregated to form a single df.\n",
    "prediction_data = pd.DataFrame({'consecutive_survey_group':all_groups,\n",
    "                                'prediction_data':prediction_data,\n",
    "                                'edof':edof})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc9910de-2ec0-42e8-9eb3-e69f4092592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving predictions to csv\n",
    "prediction_data.to_csv(project_root/'Data'/'UKBMS'/'gam_1'/'obs_12_13_14'/'obs_12_13_14_predictions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (geo_env)",
   "language": "python",
   "name": "geo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
