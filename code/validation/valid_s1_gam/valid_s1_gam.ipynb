{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0969bcd6-028f-4e31-9ebf-f6f6a175fefd",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "Before baseline indices can be predicted, it is important to optimise and validate the\n",
    "prediction model. \n",
    "Several model optimisation methods exist in Pygam (GCV, AIC, UBRE). However these\n",
    "metrics are designed for evaluating the model 'fit' over a range of values (the fit is \n",
    "optimised by balancing predictive power and complexity). The fit of the model in this \n",
    "context is not important, neither the accuracy to which the model predicts surrounding \n",
    "years, only the accuracy of its baseline year prediction. For this reason another \n",
    "optimisation workflow is selected: each model is used to predict the missing \n",
    "baseline year. In another section these predictions are grouped and compared with the \n",
    "actual values. The performance metrics are then computed for each model (correlation \n",
    "coefficient, RMSE, MSE, ME and visualisations), and the optimal model is selected \n",
    "based on these. \n",
    "\n",
    "#### Aim \n",
    "The aim of this section is to generate GAM baseline predictions using different input\n",
    "parameters (lambda and spline count). The predicitve performance of each model is \n",
    "assessed in another section. \n",
    "\n",
    "#### Workflow\n",
    "1) The dataset is log trnaformed.\n",
    "2) The dataset is filtered by sample size (A seperate model is optimised for sample\n",
    "sizes 6-8, 9-11, 12-14, 15-17, 18-20). \n",
    "3) The model input parameters are defined.\n",
    "4) A 'for loop' is used to generate models and compute predcitions. These are stored\n",
    "in a dictionary.\n",
    "5) The prediction data is cleaned and exported for performance evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8fa292d-81f1-42a5-b55f-77503c42d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pygam import LinearGAM, s \n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Importing localised file directory\n",
    "project_root = Path(os.environ['butterfly_project'])\n",
    "\n",
    "# Importing data\n",
    "gam_1_accept = pd.read_csv(project_root/'Data'/'UKBMS'/'gam_optimisation'/'gam_1_accept_validation.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348909c8-f908-4fb8-aa74-b40c770fd7c8",
   "metadata": {},
   "source": [
    "#### Log Transforming Site Index Scores Using log1p().\n",
    "log1p() is the same as log(x+1). It enables the inclusion of zero values. \n",
    "Indices are transformed due to a large right-skew in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4c8dcf-4514-4439-bfc2-a07c099224c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gam_1_accept['log_site_index'] = np.log1p(gam_1_accept['site_index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2849d4-48c4-4f59-93bc-8b73df61c312",
   "metadata": {},
   "source": [
    "#### Filtering the Dataset by Sample Size\n",
    "Survey groups are partitioned by the number of consecutive surveys. This is because \n",
    "different sample sizes are likely to require different GAM parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b54616-fc00-4f77-b049-fdbe9fa83daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gam_1_accept = (\n",
    "    gam_1_accept[\n",
    "    (gam_1_accept['consecutive_surveys']>=18) # enter min 'consecutive_surveys' here\n",
    "    & (gam_1_accept['consecutive_surveys']<=20) # enter max 'consecutive_surveys' here\n",
    "    ].reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35752bf5-e04d-4c16-89f2-8d1da9e9df67",
   "metadata": {},
   "source": [
    "#### Defining the Hyperparameters\n",
    "The parameters to be optimised are lambda and spline count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a17c69f-cc52-43f8-a889-b0e141c5cc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lam=(np.flip(np.logspace(-3,3,11))) # range of lambda values\n",
    "splines=np.arange(4,11) # range of spline values\n",
    "\n",
    "# 'product' is used to create an iterator for all combinations of the input parameters\n",
    "from itertools import product \n",
    "combinations = product(lam, splines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b000c1e-4c39-4c81-bf6f-1f0eb22e05c0",
   "metadata": {},
   "source": [
    "#### Creating a Baseline Model for All site/species Groups and Hyperparameter Combinations\n",
    "Two loops are created:\n",
    "- the outer loop runs through all the combinations of hyperparameters. These are fed into the inner loop. \n",
    "- the inner loop runs through all the combinations of 'consecutive survey groups'. These arefed directly into the GAM. This way predictions for all survey groups are created from a single combination of hyperparameters.\n",
    "- Once the inner loop is complete, the predictions are stored, and the process repeats for another combination of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33273b97-3605-4e6a-b67b-ac66a12948bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An empty dictionary is created. This will store all the model predictions.\n",
    "parameter_data = {} \n",
    "# The outer loop\n",
    "for lam, splines in combinations: \n",
    "    # Creating a unique list of all consecutive survey groups\n",
    "    all_groups = list(gam_1_accept['consecutive_survey_group'].drop_duplicates()) \n",
    "    # Empty lists are created to store the model output. \n",
    "    prediction_data = [] # becomes populated by predictions generated in inner loop.\n",
    "    edof = [] # effective degrees of freedom.\n",
    "\n",
    "    # Now the inner loop is created. This loops through all consecutive survey groups. \n",
    "    for cs_group in all_groups: \n",
    "        # gam_1_accept dataset is filtered by 'consecutive_survey_group' in each loop\n",
    "        cs_dataset = (\n",
    "            gam_1_accept[\n",
    "            gam_1_accept['consecutive_survey_group']==cs_group\n",
    "            ]\n",
    "        )\n",
    "        model_all = (\n",
    "            LinearGAM(s(0, n_splines=splines), lam=lam)\n",
    "            .fit(cs_dataset[['year']], # x-axis years specific to 'cs_dataset'\n",
    "                 cs_dataset['log_site_index']) # y-axis indices specific to cs_dataset.\n",
    "        )\n",
    "        # Defining the baseline year in a 1x1 df. This is needed because .predict()\n",
    "        # only accepts 2D input. \n",
    "        explanatory_data = pd.DataFrame([1993], columns=['explanatory_data'])\n",
    "        # using the model to generate a single baseline prediction for each cs_group\n",
    "        prediction_data.append(\n",
    "            model_all.predict(explanatory_data)[0] # [0] extracts value from 1D array\n",
    "        )\n",
    "        # the edof for each model is extracted\n",
    "        edof.append(model_all.statistics_['edof'])\n",
    "\n",
    "    # At the end of every parameter loop, populated lists are aggregated to form a \n",
    "    # single df.\n",
    "    prediction_data = pd.DataFrame({'consecutive_survey_group':all_groups,\n",
    "                                    'prediction_data':prediction_data, \n",
    "                                    'edof':edof})\n",
    "    # Saving each df of parameter-specific predictions in a dictionary. \n",
    "    # The key of each df is the parameters used in the model. \n",
    "    parameter_data[(lam, splines)] = prediction_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37edcf0f-b6b1-49b3-92dd-a0a7e5c9926e",
   "metadata": {},
   "source": [
    "#### Cleaning Prediction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f8225a5-841c-4811-a017-90cfb40da53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compare baseline prediction data with the actual data, site/species combinations \n",
    "# will need to be referenced. For this reason site and species code columns are added. \n",
    "for key, value in parameter_data.items():\n",
    "    parameter_data[key] = (\n",
    "        value.merge(\n",
    "            gam_1_accept[[\n",
    "                'species_code',\n",
    "                'site_code',\n",
    "                'consecutive_survey_group']]\n",
    "            # Each consecutive survey group is formed of multiple years. Only unique \n",
    "            # consecutive survey group numbers are required.\n",
    "            .drop_duplicates(), \n",
    "            on='consecutive_survey_group', \n",
    "            how='inner')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b59a5539-4004-4e08-8a8f-82f4b4985ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting logspace values to log base 10 for improved readability.\n",
    "parameter_data_cleaned = {}\n",
    "# Loops through all keys and values in 'parameter_data' dictionary.\n",
    "for (lam, splines), value in parameter_data.items(): \n",
    "    # np.log(10) is used for the transformation. Data is stored in new dictionary. \n",
    "    parameter_data_cleaned[(round(np.log10(lam),1),splines)] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc9910de-2ec0-42e8-9eb3-e69f4092592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting dictionary\n",
    "import pickle\n",
    "with open(project_root/'Data'/'UKBMS'/'gam_optimisation'/'obs_18_19_20'/'obs_18_19_20_parameters.pkl','wb') as file:\n",
    "    pickle.dump(parameter_data_cleaned, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (geo_env)",
   "language": "python",
   "name": "geo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
